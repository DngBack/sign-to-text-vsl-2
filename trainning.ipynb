{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional,GlobalAveragePooling1D, Activation\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\auth\\__init__.py:54: FutureWarning: \n",
            "    You are using a Python version 3.9 past its end of life. Google will update\n",
            "    google-auth with critical bug fixes on a best-effort basis, but not\n",
            "    with any other fixes or features. Please upgrade your Python version,\n",
            "    and then update google-auth.\n",
            "    \n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\oauth2\\__init__.py:40: FutureWarning: \n",
            "    You are using a Python version 3.9 past its end of life. Google will update\n",
            "    google-auth with critical bug fixes on a best-effort basis, but not\n",
            "    with any other fixes or features. Please upgrade your Python version,\n",
            "    and then update google-auth.\n",
            "    \n",
            "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
          ]
        }
      ],
      "id": "0fe98b62"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Config\n",
        "DATA_PATH        = 'Data'\n",
        "LABEL_MAP_PATH   = 'Logs/label_map.json'\n",
        "BATCH_SIZE       = 32\n",
        "AUTOTUNE         = tf.data.AUTOTUNE\n",
        "VAL_SPLIT        = 0.1\n",
        "TEST_SPLIT       = 0.1\n",
        "\n",
        "# 2. Load label_map from JSON\n",
        "with open(LABEL_MAP_PATH, 'r', encoding='utf-8') as f:\n",
        "    label_map = json.load(f)\n",
        "\n",
        "NUM_CLASSES = len(label_map)\n",
        "print(f\"Number of classes: {NUM_CLASSES}\")\n",
        "\n",
        "# 3. List all .npz files (Data/<action_name>/*.npz)\n",
        "file_pattern = os.path.join(DATA_PATH, '**', '*.npz')\n",
        "all_files = glob.glob(file_pattern, recursive=True)\n",
        "print(f\"Found {len(all_files)} samples.\")\n",
        "\n",
        "# Stratify by class (folder name) so train/val/test have similar class distribution\n",
        "stratify_labels = [os.path.basename(os.path.dirname(p)) for p in all_files]\n",
        "train_files, temp_files = train_test_split(\n",
        "    all_files,\n",
        "    test_size=VAL_SPLIT + TEST_SPLIT,\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=stratify_labels\n",
        ")\n",
        "\n",
        "stratify_temp = [os.path.basename(os.path.dirname(p)) for p in temp_files]\n",
        "val_files, test_files = train_test_split(\n",
        "    temp_files,\n",
        "    test_size=TEST_SPLIT / (VAL_SPLIT + TEST_SPLIT),\n",
        "    shuffle=True,\n",
        "    random_state=42,\n",
        "    stratify=stratify_temp\n",
        ")\n",
        "\n",
        "print(f\"  Train samples: {len(train_files)}\")\n",
        "print(f\"    Val samples: {len(val_files)}\")\n",
        "print(f\"   Test samples: {len(test_files)}\")\n",
        "\n",
        "# 4. Parse each .npz file\n",
        "def _load_npz(path):\n",
        "    npz_path = path.decode('utf-8')\n",
        "    data = np.load(npz_path)\n",
        "    seq = data['sequence'].astype(np.float32)\n",
        "    lbl = np.int32(data['label'])\n",
        "    return seq, lbl\n",
        "\n",
        "def parse_fn(path):\n",
        "    seq, lbl = tf.numpy_function(\n",
        "        func=_load_npz,\n",
        "        inp=[path],\n",
        "        Tout=[tf.float32, tf.int32]\n",
        "    )\n",
        "    seq.set_shape([60, 201])\n",
        "    lbl.set_shape([])\n",
        "    return seq, lbl\n",
        "def make_dataset(file_list, shuffle=False, repeat=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(file_list)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(len(file_list), reshuffle_each_iteration=True)\n",
        "    if repeat:\n",
        "        ds = ds.repeat()\n",
        "    ds = ds.map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
        "    ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    ds = ds.prefetch(AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# 6. Tạo train_ds & val_ds\n",
        "train_ds = make_dataset(train_files, shuffle=True, repeat=True)\n",
        "val_ds   = make_dataset(val_files, shuffle=False, repeat=False)\n",
        "test_ds  = make_dataset(test_files, shuffle=False, repeat=False)\n",
        "\n",
        "# 7. Compute steps\n",
        "steps_per_epoch = len(train_files) // BATCH_SIZE\n",
        "validation_steps = len(val_files) // BATCH_SIZE"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5049 samples.\n",
            "  Train samples: 4039\n",
            "    Val samples: 505\n",
            "   Test samples: 505\n"
          ]
        }
      ],
      "id": "6d1ab571"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# When number of classes grows (100 -> 500 -> 1000+), use larger model for better discrimination.\n",
        "# Below 500 classes: standard size; 500+: large model.\n",
        "USE_LARGE_MODEL = NUM_CLASSES >= 500\n",
        "lstm_units = 384 if USE_LARGE_MODEL else 256\n",
        "dense_1, dense_2 = (768, 384) if USE_LARGE_MODEL else (512, 256)\n",
        "if USE_LARGE_MODEL:\n",
        "    print(\"Using LARGE model (LSTM 384, Dense 768->384) for many classes.\")\n",
        "\n",
        "inputs = tf.keras.Input(shape=(60, 201))\n",
        "\n",
        "# LSTM blocks\n",
        "x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.3))(inputs)\n",
        "x = BatchNormalization()(x)\n",
        "x = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.3))(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Bidirectional(LSTM(lstm_units, dropout=0.3))(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Dense layers\n",
        "x = Dense(dense_1, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(dense_2, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = BatchNormalization()(x)\n",
        "\n",
        "# Output layer: number of units = number of classes (from label_map)\n",
        "outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Label smoothing helps when there are many classes (less overconfident, better generalization)\n",
        "LABEL_SMOOTHING = 0.1\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [],
      "id": "8adb8e7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Checkpoint directory\n",
        "checkpoint_dir = 'Models/checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, 'final_model.keras')\n",
        "\n",
        "# 2. Callbacks\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    ),\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    )\n",
        "]"
      ],
      "execution_count": 4,
      "outputs": [],
      "id": "47ae33f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false
      },
      "source": [
        "model.fit(\n",
        "    train_ds,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks = callbacks\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.0355 - loss: 7.1225\n",
            "Epoch 1: val_loss improved from inf to 3.77955, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 2s/step - accuracy: 0.0357 - loss: 7.1121 - val_accuracy: 0.1125 - val_loss: 3.7796\n",
            "Epoch 2/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 2s/step - accuracy: 0.1600 - loss: 3.0988\n",
            "Epoch 3/100\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\callbacks\\model_checkpoint.py:206: UserWarning: Can save best model only with val_loss available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n",
            "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2868 - loss: 2.3785\n",
            "Epoch 3: val_loss improved from 3.77955 to 1.79479, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 2s/step - accuracy: 0.2870 - loss: 2.3777 - val_accuracy: 0.4271 - val_loss: 1.7948\n",
            "Epoch 4/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.3870 - loss: 1.9422\n",
            "Epoch 5/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4507 - loss: 1.6407\n",
            "Epoch 5: val_loss improved from 1.79479 to 1.57072, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 2s/step - accuracy: 0.4508 - loss: 1.6400 - val_accuracy: 0.4875 - val_loss: 1.5707\n",
            "Epoch 6/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 2s/step - accuracy: 0.5283 - loss: 1.3803\n",
            "Epoch 7/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5891 - loss: 1.1196\n",
            "Epoch 7: val_loss improved from 1.57072 to 0.70064, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 2s/step - accuracy: 0.5892 - loss: 1.1194 - val_accuracy: 0.7667 - val_loss: 0.7006\n",
            "Epoch 8/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 2s/step - accuracy: 0.6486 - loss: 0.9633\n",
            "Epoch 9/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6953 - loss: 0.8388\n",
            "Epoch 9: val_loss improved from 0.70064 to 0.46770, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 2s/step - accuracy: 0.6953 - loss: 0.8387 - val_accuracy: 0.8167 - val_loss: 0.4677\n",
            "Epoch 10/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 2s/step - accuracy: 0.7502 - loss: 0.6957\n",
            "Epoch 11/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7959 - loss: 0.5691\n",
            "Epoch 11: val_loss improved from 0.46770 to 0.38943, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 2s/step - accuracy: 0.7959 - loss: 0.5691 - val_accuracy: 0.8313 - val_loss: 0.3894\n",
            "Epoch 12/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 2s/step - accuracy: 0.8100 - loss: 0.5328\n",
            "Epoch 13/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8500 - loss: 0.4288\n",
            "Epoch 13: val_loss improved from 0.38943 to 0.29018, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 2s/step - accuracy: 0.8500 - loss: 0.4288 - val_accuracy: 0.8792 - val_loss: 0.2902\n",
            "Epoch 14/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 2s/step - accuracy: 0.8393 - loss: 0.4597\n",
            "Epoch 15/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8799 - loss: 0.3399\n",
            "Epoch 15: val_loss improved from 0.29018 to 0.09012, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 2s/step - accuracy: 0.8799 - loss: 0.3397 - val_accuracy: 0.9771 - val_loss: 0.0901\n",
            "Epoch 16/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 2s/step - accuracy: 0.8965 - loss: 0.3068\n",
            "Epoch 17/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.8915 - loss: 0.3023\n",
            "Epoch 17: val_loss improved from 0.09012 to 0.07090, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 2s/step - accuracy: 0.8915 - loss: 0.3023 - val_accuracy: 0.9771 - val_loss: 0.0709\n",
            "Epoch 18/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 2s/step - accuracy: 0.9145 - loss: 0.2498\n",
            "Epoch 19/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9163 - loss: 0.2363\n",
            "Epoch 19: val_loss improved from 0.07090 to 0.05591, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 2s/step - accuracy: 0.9164 - loss: 0.2363 - val_accuracy: 0.9896 - val_loss: 0.0559\n",
            "Epoch 20/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 3s/step - accuracy: 0.9299 - loss: 0.2053\n",
            "Epoch 21/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.9390 - loss: 0.1882\n",
            "Epoch 21: val_loss improved from 0.05591 to 0.02565, saving model to Models/checkpoints\\final_model.keras\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 3s/step - accuracy: 0.9391 - loss: 0.1881 - val_accuracy: 0.9937 - val_loss: 0.0257\n",
            "Epoch 22/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 2s/step - accuracy: 0.9438 - loss: 0.1757\n",
            "Epoch 23/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9168 - loss: 0.2588\n",
            "Epoch 23: val_loss did not improve from 0.02565\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m301s\u001b[0m 2s/step - accuracy: 0.9169 - loss: 0.2584 - val_accuracy: 0.9896 - val_loss: 0.0301\n",
            "Epoch 24/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 2s/step - accuracy: 0.9500 - loss: 0.1626\n",
            "Epoch 25/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9474 - loss: 0.1638\n",
            "Epoch 25: val_loss did not improve from 0.02565\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 2s/step - accuracy: 0.9475 - loss: 0.1637 - val_accuracy: 0.9833 - val_loss: 0.0574\n",
            "Epoch 26/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 2s/step - accuracy: 0.9608 - loss: 0.1236\n",
            "Epoch 27/100\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.9639 - loss: 0.1274\n",
            "Epoch 27: val_loss did not improve from 0.02565\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 2s/step - accuracy: 0.9638 - loss: 0.1275 - val_accuracy: 0.9896 - val_loss: 0.0414\n",
            "Epoch 28/100\n",
            "\u001b[1m 54/126\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 2s/step - accuracy: 0.9600 - loss: 0.1232"
          ]
        }
      ],
      "id": "ca557bcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "291a9e5b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "execution_count": null,
      "outputs": [],
      "id": "8dec7778"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}