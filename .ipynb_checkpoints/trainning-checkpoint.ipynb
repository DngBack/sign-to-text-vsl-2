{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe98b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:246: FutureWarning: You are using a non-supported Python version (3.9.10). Google will not post any further updates to google.api_core supporting this Python version. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\auth\\__init__.py:54: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\oauth2\\__init__.py:40: FutureWarning: \n",
      "    You are using a Python version 3.9 past its end of life. Google will update\n",
      "    google-auth with critical bug fixes on a best-effort basis, but not\n",
      "    with any other fixes or features. Please upgrade your Python version,\n",
      "    and then update google-auth.\n",
      "    \n",
      "  warnings.warn(eol_message.format(\"3.9\"), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional,GlobalAveragePooling1D, Activation\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1ab571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10200 samples.\n",
      "  Train samples: 8160\n",
      "    Val samples: 1020\n",
      "   Test samples: 1020\n"
     ]
    }
   ],
   "source": [
    "# 1. Các biến cấu hình\n",
    "DATA_PATH        = 'Data'                # thư mục gốc chứa các folder action\n",
    "LABEL_MAP_PATH   = 'Logs/label_map.json'\n",
    "BATCH_SIZE       = 32\n",
    "AUTOTUNE         = tf.data.AUTOTUNE\n",
    "VAL_SPLIT        = 0.1\n",
    "TEST_SPLIT       = 0.1\n",
    "# 2. Load label_map từ JSON\n",
    "with open(LABEL_MAP_PATH, 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)            # ví dụ: {\"địa chỉ\": 0, \"miến điện\": 1, ...}\n",
    "\n",
    "# 3. Tạo danh sách tất cả các file .npz\n",
    "#    Data structure: Data/<action_name>/*.npz\n",
    "file_pattern = os.path.join(DATA_PATH, '**', '*.npz')\n",
    "all_files = glob.glob(file_pattern, recursive=True)\n",
    "print(f\"Found {len(all_files)} samples.\")\n",
    "\n",
    "train_files, temp_files = train_test_split(\n",
    "    all_files,\n",
    "    test_size=VAL_SPLIT + TEST_SPLIT,  # ví dụ: 0.2 + 0.1 = 0.3\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=[os.path.basename(p).split('.')[0] for p in all_files]\n",
    ")\n",
    "\n",
    "val_files, test_files = train_test_split(\n",
    "    temp_files,\n",
    "    test_size=TEST_SPLIT / (VAL_SPLIT + TEST_SPLIT),  # ví dụ: 0.1 / 0.3 = 1/3\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    stratify=[os.path.basename(p).split('.')[0] for p in temp_files]\n",
    ")\n",
    "\n",
    "print(f\"  Train samples: {len(train_files)}\")\n",
    "print(f\"    Val samples: {len(val_files)}\")\n",
    "print(f\"   Test samples: {len(test_files)}\")\n",
    "\n",
    "# 4. Hàm parse mỗi file .npz\n",
    "def _load_npz(path):\n",
    "    # path: scalar tf.string tensor\n",
    "    npz_path = path.decode('utf-8')\n",
    "    data = np.load(npz_path)\n",
    "    seq   = data['sequence'].astype(np.float32)   # (60,126)\n",
    "    lbl   = np.int32(data['label'])\n",
    "    return seq, lbl\n",
    "\n",
    "def parse_fn(path):\n",
    "    seq, lbl = tf.numpy_function(\n",
    "        func=_load_npz,\n",
    "        inp=[path],\n",
    "        Tout=[tf.float32, tf.int32]\n",
    "    )\n",
    "    # set shape để TF biết kích thước cố định\n",
    "    seq.set_shape([60, 201])\n",
    "    lbl.set_shape([])\n",
    "    return seq, lbl\n",
    "def make_dataset(file_list, shuffle=False, repeat=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(file_list)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(file_list), reshuffle_each_iteration=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    ds = ds.map(parse_fn, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE, drop_remainder=True)\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# 6. Tạo train_ds & val_ds\n",
    "train_ds = make_dataset(train_files, shuffle=True, repeat=True)\n",
    "val_ds   = make_dataset(val_files, shuffle=False, repeat=False)\n",
    "test_ds  = make_dataset(test_files, shuffle=False, repeat=False)\n",
    "\n",
    "# 7. Compute steps\n",
    "steps_per_epoch = len(train_files) // BATCH_SIZE\n",
    "validation_steps = len(val_files) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8adb8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(60, 201))\n",
    "\n",
    "# Khối LSTM thứ nhất\n",
    "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Khối LSTM thứ hai\n",
    "x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Khối LSTM thứ ba\n",
    "x = Bidirectional(LSTM(256, dropout=0.3))(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Các lớp Dense\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "# Lớp đầu ra\n",
    "outputs = Dense(2764, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "# Biên dịch mô hình\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ae33f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tạo thư mục lưu checkpoint (nếu chưa có)\n",
    "checkpoint_dir = 'Models/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'final_model.keras')\n",
    "\n",
    "# 2. Khởi tạo callbacks\n",
    "callbacks = [\n",
    "    # Lưu mô hình với val_loss thấp nhất\n",
    "    ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,  # lưu cả kiến trúc + weights\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Dừng training nếu 5 epoch liên tiếp không cải thiện val_loss\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca557bcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m 27/255\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11:53\u001b[0m 3s/step - accuracy: 0.0018 - loss: 7.9733"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=100,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a9e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dec7778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
